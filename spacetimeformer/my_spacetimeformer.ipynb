{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeomss/miniconda3/envs/spacetimeformer/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "import spacetimeformer as stf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python train.py spacetimeformer solar_energy --context_points 168 --target_points 24 --d_model 100 --d_ff 400 --enc_layers 5 --dec_layers 5 --l2_coeff 1e-3 --dropout_ff .2 --dropout_emb .1 --d_qk 20 --d_v 20 --n_heads 6 --run_name spatiotemporal_al_solar --batch_size 32 --class_loss_imp 0 --initial_downsample_convs 1 --decay_factor .8 --warmup_steps 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is conceptually similar to [Informer](https://arxiv.org/abs/2012.07436). Set the `embed_method = temporal`. Spacetimeformer has many configurable options and we try to provide a thorough explanation with the commandline `-h` instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =[\"spacetimeformer\"]\n",
    "\n",
    "DSETS = [\"solar_energy\"]\n",
    "data_path = \"./data/solar_AL_converted.csv\"\n",
    "target_cols = [str(i) for i in range(137)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `solar_energy`: Is the codebase's name for the time series benchmark more commonly called \"AL Solar.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser():\n",
    "    # model = sys.argv[1] # spacetimeformer\n",
    "    # dset = sys.argv[2] # asos\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--model\" ,type=str, default = \"spacetimeformer\")\n",
    "    parser.add_argument(\"--dset\", type=str, default= \"asos\")\n",
    "\n",
    "    #only use asos\n",
    "    stf.data.CSVTimeSeries.add_cli(parser) #parser.add_argument(\"--data_path\", type=str, default=\"auto\")\n",
    "    stf.data.CSVTorchDset.add_cli(parser) # context, target points, time resolution parser\n",
    "    stf.data.DataModule.add_cli(parser) # batchsize, worker, overfit\n",
    "\n",
    "    # spacetimeformer\n",
    "    stf.spacetimeformer_model.Spacetimeformer_Forecaster.add_cli(parser)\n",
    "    \n",
    "\n",
    "    stf.callbacks.TimeMaskedLossCallback.add_cli(parser)\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--wandb\", action=\"store_true\")\n",
    "    parser.add_argument(\"--plot\", action=\"store_true\")\n",
    "    parser.add_argument(\"--plot_samples\", type=int, default=8)\n",
    "    parser.add_argument(\"--attn_plot\", action=\"store_true\")\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "    parser.add_argument(\"--run_name\", type=str, required=True)\n",
    "    parser.add_argument(\"--accumulate\", type=int, default=1)\n",
    "    parser.add_argument(\"--val_check_interval\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--limit_val_batches\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--no_earlystopping\", action=\"store_true\")\n",
    "    parser.add_argument(\"--patience\", type=int, default=5)\n",
    "    parser.add_argument(\n",
    "        \"--trials\", type=int, default=1, help=\"How many consecutive trials to run\"\n",
    "    )\n",
    "\n",
    "    if len(sys.argv) > 3 and sys.argv[3] == \"-h\":\n",
    "        parser.print_help()\n",
    "        sys.exit(0)\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "\n",
    "    #as to solar energy dataset\n",
    "    x_dim, yc_dim, yt_dim = None, None, None\n",
    "    x_dim = 6\n",
    "    yc_dim = 137\n",
    "    yt_dim = 137\n",
    "\n",
    "\n",
    "    #config.model == \"spacetimeformer\":\n",
    "\n",
    "    #setting for target length and context length\n",
    "    if hasattr(config, \"context_points\") and hasattr(config, \"target_points\"):\n",
    "        max_seq_len = config.context_points + config.target_points\n",
    "    elif hasattr(config, \"max_len\"):\n",
    "        max_seq_len = config.max_len\n",
    "    else:\n",
    "        raise ValueError(\"Undefined max_seq_len\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    forecaster = stf.spacetimeformer_model.Spacetimeformer_Forecaster(\n",
    "        d_x=x_dim,\n",
    "        d_yc=yc_dim,\n",
    "        d_yt=yt_dim,\n",
    "        max_seq_len=max_seq_len,\n",
    "        start_token_len=config.start_token_len,\n",
    "\n",
    "\n",
    "        attn_factor=config.attn_factor,\n",
    "        d_model=config.d_model,\n",
    "        d_queries_keys=config.d_qk,\n",
    "        d_values=config.d_v,\n",
    "        n_heads=config.n_heads,\n",
    "        e_layers=config.enc_layers,\n",
    "        d_layers=config.dec_layers,\n",
    "        d_ff=config.d_ff,\n",
    "        dropout_emb=config.dropout_emb,\n",
    "        dropout_attn_out=config.dropout_attn_out,\n",
    "        dropout_attn_matrix=config.dropout_attn_matrix,\n",
    "        dropout_qkv=config.dropout_qkv,\n",
    "        dropout_ff=config.dropout_ff,\n",
    "        pos_emb_type=config.pos_emb_type,\n",
    "        use_final_norm=not config.no_final_norm,\n",
    "        global_self_attn=config.global_self_attn,\n",
    "        local_self_attn=config.local_self_attn,\n",
    "        global_cross_attn=config.global_cross_attn,\n",
    "        local_cross_attn=config.local_cross_attn,\n",
    "        performer_kernel=config.performer_kernel,\n",
    "        performer_redraw_interval=config.performer_redraw_interval,\n",
    "        attn_time_windows=config.attn_time_windows,\n",
    "        use_shifted_time_windows=config.use_shifted_time_windows,\n",
    "        norm=config.norm,\n",
    "        activation=config.activation,\n",
    "        init_lr=config.init_lr,\n",
    "        base_lr=config.base_lr,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        decay_factor=config.decay_factor,\n",
    "        initial_downsample_convs=config.initial_downsample_convs,\n",
    "        intermediate_downsample_convs=config.intermediate_downsample_convs,\n",
    "        embed_method=config.embed_method,\n",
    "        l2_coeff=config.l2_coeff,\n",
    "        loss=config.loss,\n",
    "        class_loss_imp=config.class_loss_imp,\n",
    "        recon_loss_imp=config.recon_loss_imp,\n",
    "        time_emb_dim=config.time_emb_dim,\n",
    "        null_value=config.null_value,\n",
    "        pad_value=config.pad_value,\n",
    "        linear_window=config.linear_window,\n",
    "        use_revin=config.use_revin,\n",
    "        linear_shared_weights=config.linear_shared_weights,\n",
    "        use_seasonal_decomp=config.use_seasonal_decomp,\n",
    "        use_val=not config.no_val,\n",
    "        use_time=not config.no_time,\n",
    "        use_space=not config.no_space,\n",
    "        use_given=not config.no_given,\n",
    "        recon_mask_skip_all=config.recon_mask_skip_all,\n",
    "        recon_mask_max_seq_len=config.recon_mask_max_seq_len,\n",
    "        recon_mask_drop_seq=config.recon_mask_drop_seq,\n",
    "        recon_mask_drop_standard=config.recon_mask_drop_standard,\n",
    "        recon_mask_drop_full=config.recon_mask_drop_full,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    return forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dset(config):\n",
    "    INV_SCALER = lambda x: x\n",
    "    SCALER = lambda x: x\n",
    "    NULL_VAL = None\n",
    "    PLOT_VAR_IDXS = None\n",
    "    PLOT_VAR_NAMES = None\n",
    "    PAD_VAL = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    time_col_name = \"Datetime\"\n",
    "    data_path = config.data_path\n",
    "    time_features = [\"year\", \"month\", \"day\", \"weekday\", \"hour\", \"minute\"]\n",
    "\n",
    "    \n",
    "    if config.dset == \"asos\":\n",
    "        if data_path == \"auto\":\n",
    "            data_path = \"./data/temperature-v1.csv\"\n",
    "        target_cols = [\"ABI\", \"AMA\", \"ACT\", \"ALB\", \"JFK\", \"LGA\"]\n",
    "    \n",
    "    elif config.dset == \"solar_energy\":\n",
    "        if data_path == \"auto\":\n",
    "            data_path = \"./data/solar_AL_converted.csv\"\n",
    "        target_cols = [str(i) for i in range(137)]\n",
    "  \n",
    "\n",
    "    dset = stf.data.CSVTimeSeries(\n",
    "        data_path=data_path,\n",
    "        target_cols=target_cols,\n",
    "        ignore_cols=\"all\",\n",
    "        time_col_name=time_col_name,\n",
    "        time_features=time_features,\n",
    "        val_split=0.2,\n",
    "        test_split=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    DATA_MODULE = stf.data.DataModule(\n",
    "        datasetCls=stf.data.CSVTorchDset,\n",
    "        dataset_kwargs={\n",
    "            \"csv_time_series\": dset,\n",
    "            \"context_points\": config.context_points,\n",
    "            \"target_points\": config.target_points,\n",
    "            \"time_resolution\": config.time_resolution,\n",
    "        },\n",
    "        batch_size=config.batch_size,\n",
    "        workers=config.workers,\n",
    "        overfit=config.overfit, # arg는 from global parser\n",
    "    )\n",
    "\n",
    "\n",
    "    ##전처리 하는 것.\n",
    "    INV_SCALER = dset.reverse_scaling\n",
    "    SCALER = dset.apply_scaling\n",
    "    NULL_VAL = None\n",
    "\n",
    "\n",
    "    return (\n",
    "    DATA_MODULE,\n",
    "    INV_SCALER,\n",
    "    SCALER,\n",
    "    NULL_VAL,\n",
    "    PLOT_VAR_IDXS,\n",
    "    PLOT_VAR_NAMES,\n",
    "    PAD_VAL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pytorch lightning callback\n",
    "    - 마지막 epoch 체크 포인트 저장이 아니라, 매 epoch마다 저장하는 등\n",
    "    - 세부적으로 저장할 때 쓰는 모듈임. : ModelCheckpoint\n",
    "    - wandb : wieghts & Biases\n",
    "    - time_mask_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(config, save_dir):\n",
    "    filename = f\"{config.run_name}_\" + str(uuid.uuid1()).split(\"-\")[0]\n",
    "    model_ckpt_dir = os.path.join(save_dir, filename)\n",
    "    config.model_ckpt_dir = model_ckpt_dir\n",
    "    \n",
    "    \n",
    "    saving = pl.callbacks.ModelCheckpoint(\n",
    "        dirpath=model_ckpt_dir,\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "        filename=f\"{config.run_name}\" + \"{epoch:02d}\",\n",
    "        save_top_k=1,\n",
    "        auto_insert_metric_name=True,\n",
    "    )\n",
    "    callbacks = [saving]\n",
    "\n",
    "    if not config.no_earlystopping:\n",
    "        callbacks.append(\n",
    "            pl.callbacks.early_stopping.EarlyStopping(\n",
    "                monitor=\"val/loss\",\n",
    "                patience=config.patience,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if config.wandb:\n",
    "        callbacks.append(pl.callbacks.LearningRateMonitor())\n",
    "\n",
    "    if config.model == \"lstm\":\n",
    "        callbacks.append(\n",
    "            stf.callbacks.TeacherForcingAnnealCallback(\n",
    "                start=config.teacher_forcing_start,\n",
    "                end=config.teacher_forcing_end,\n",
    "                steps=config.teacher_forcing_anneal_steps,\n",
    "            )\n",
    "        )\n",
    "    if config.time_mask_loss:\n",
    "        callbacks.append(\n",
    "            stf.callbacks.TimeMaskedLossCallback(\n",
    "                start=config.time_mask_start,\n",
    "                end=config.time_mask_end,\n",
    "                steps=config.time_mask_anneal_steps,\n",
    "            )\n",
    "        )\n",
    "    return callbacks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Wandb**  :MLOps Tools로 모델 학습 추적을 하고, 더 나은 모델을 빨리 만들어주는 머신러닝 툴이다.\n",
    "* 실험과정 Tracking, 시각화 툴임.\n",
    "* Neptune AI랑 비슷한 결이라고 생각하면 될듯!\n",
    "\n",
    "이거 없이 돌린는 코드만 짜면 될듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    log_dir = os.getenv(\"STF_LOG_DIR\")\n",
    "    if log_dir is None:\n",
    "        log_dir = \"./data/STF_LOG_DIR\"\n",
    "        print(\n",
    "            \"Using default wandb log dir path of ./data/STF_LOG_DIR. This can be adjusted with the environment variable `STF_LOG_DIR`\"\n",
    "        )\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "    #여기는 그냥 주석 처리 혹은 wandb = False 처리\n",
    "    # if args.wandb:\n",
    "    #     import wandb\n",
    "\n",
    "    #     project = os.getenv(\"STF_WANDB_PROJ\")\n",
    "    #     entity = os.getenv(\"STF_WANDB_ACCT\")\n",
    "    #     assert (\n",
    "    #         project is not None and entity is not None\n",
    "    #     ), \"Please set environment variables `STF_WANDB_ACCT` and `STF_WANDB_PROJ` with \\n\\\n",
    "    #         your wandb user/organization name and project title, respectively.\"\n",
    "    #     experiment = wandb.init(\n",
    "    #         project=project,\n",
    "    #         entity=entity,\n",
    "    #         config=args,\n",
    "    #         dir=log_dir,\n",
    "    #         reinit=True,\n",
    "    #     )\n",
    "    #     config = wandb.config\n",
    "    #     wandb.run.name = args.run_name\n",
    "    #     wandb.run.save()\n",
    "    #     logger = pl.loggers.WandbLogger(\n",
    "    #         experiment=experiment,\n",
    "    #         save_dir=log_dir,\n",
    "    #     )\n",
    "\n",
    "    # Dset\n",
    "    (\n",
    "        data_module,\n",
    "        inv_scaler,\n",
    "        scaler,\n",
    "        null_val,\n",
    "        plot_var_idxs,\n",
    "        plot_var_names,\n",
    "        pad_val,\n",
    "    ) = create_dset(args)\n",
    "\n",
    "\n",
    "\n",
    "    # Model\n",
    "    args.null_value = null_val\n",
    "    args.pad_value = pad_val\n",
    "    forecaster = create_model(args)\n",
    "    forecaster.set_inv_scaler(inv_scaler)\n",
    "    forecaster.set_scaler(scaler)\n",
    "    forecaster.set_null_value(null_val)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = create_callbacks(args, save_dir=log_dir)\n",
    "    test_samples = next(iter(data_module.test_dataloader())) #한 sample만 가져오기\n",
    "\n",
    "\n",
    "    #plotting\n",
    "    if args.wandb and args.plot:\n",
    "        callbacks.append(\n",
    "            stf.plot.PredictionPlotterCallback(\n",
    "                test_samples,\n",
    "                var_idxs=plot_var_idxs,\n",
    "                var_names=plot_var_names,\n",
    "                pad_val=pad_val,\n",
    "                total_samples=min(args.plot_samples, args.batch_size),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # if args.wandb and args.dset in [\"mnist\", \"cifar\"] and args.plot:\n",
    "    #     callbacks.append(\n",
    "    #         stf.plot.ImageCompletionCallback(\n",
    "    #             test_samples,\n",
    "    #             total_samples=min(16, args.batch_size),\n",
    "    #             mode=\"left-right\" if config.dset == \"mnist\" else \"flat\",\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    # if args.wandb and args.dset == \"copy\" and args.plot:\n",
    "    #     callbacks.append(\n",
    "    #         stf.plot.CopyTaskCallback(\n",
    "    #             test_samples,\n",
    "    #             total_samples=min(16, args.batch_size),\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    if args.wandb and args.model == \"spacetimeformer\" and args.attn_plot:\n",
    "\n",
    "        callbacks.append(\n",
    "            stf.plot.AttentionMatrixCallback(\n",
    "                test_samples,\n",
    "                layer=0,\n",
    "                total_samples=min(16, args.batch_size),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # if args.wandb:\n",
    "    #     config.update(args)\n",
    "    #     logger.log_hyperparams(config)\n",
    "\n",
    "    if args.val_check_interval <= 1.0:\n",
    "        val_control = {\"val_check_interval\": args.val_check_interval}\n",
    "    else:\n",
    "        val_control = {\"check_val_every_n_epoch\": int(args.val_check_interval)}\n",
    "\n",
    "\n",
    "    #pytorch lighting의 모델 정의 부분\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=args.gpus,\n",
    "        callbacks=callbacks,\n",
    "        # logger=logger if args.wandb else None,\n",
    "        accelerator=\"dp\",\n",
    "        gradient_clip_val=args.grad_clip_norm,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        overfit_batches=20 if args.debug else 0,\n",
    "        accumulate_grad_batches=args.accumulate,\n",
    "        sync_batchnorm=True,\n",
    "        limit_val_batches=args.limit_val_batches,\n",
    "        **val_control,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.fit(forecaster, datamodule=data_module)\n",
    "\n",
    "    # Test\n",
    "    trainer.test(datamodule=data_module, ckpt_path=\"best\")\n",
    "\n",
    "    # Predict (only here as a demo and test)\n",
    "    # forecaster.to(\"cuda\")\n",
    "    # xc, yc, xt, _ = test_samples\n",
    "    # yt_pred = forecaster.predict(xc, yc, xt)\n",
    "\n",
    "    # if args.wandb:\n",
    "    #     experiment.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**main run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(accumulate=1, activation='gelu', attn_factor=5, attn_plot=False, attn_time_windows=1, base_lr=0.0005, batch_size=32, class_loss_imp=0.0, context_points=168, d_ff=400, d_model=100, d_qk=20, d_v=20, data_path='auto', debug=False, dec_layers=5, decay_factor=0.8, dropout_attn_matrix=0.0, dropout_attn_out=0.0, dropout_emb=0.1, dropout_ff=0.2, dropout_qkv=0.0, dset='solar_energy', embed_method='spatio-temporal', enc_layers=5, global_cross_attn='performer', global_self_attn='performer', gpus=None, grad_clip_norm=0, init_lr=1e-10, initial_downsample_convs=1, intermediate_downsample_convs=0, l2_coeff=0.001, learning_rate=0.0001, limit_val_batches=1.0, linear_shared_weights=False, linear_window=0, local_cross_attn='performer', local_self_attn='performer', loss='mse', model='spacetimeformer', n_heads=6, no_earlystopping=False, no_final_norm=False, no_given=False, no_space=False, no_time=False, no_val=False, norm='batch', overfit=False, patience=5, performer_kernel='relu', performer_redraw_interval=100, plot=False, plot_samples=8, pos_emb_type='abs', recon_loss_imp=0.0, recon_mask_drop_full=0.05, recon_mask_drop_seq=0.2, recon_mask_drop_standard=0.1, recon_mask_max_seq_len=5, recon_mask_skip_all=1.0, run_name='spatiotemporal_al_solar', start_token_len=0, target_points=24, time_emb_dim=6, time_mask_anneal_steps=1000, time_mask_end=12, time_mask_loss=False, time_mask_start=1, time_resolution=1, trials=1, use_revin=False, use_seasonal_decomp=False, use_shifted_time_windows=False, val_check_interval=1.0, wandb=False, warmup_steps=1000, workers=6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parsing args\n",
    "parser = create_parser()\n",
    "args = \"--model spacetimeformer --dset solar_energy --context_points 168 --target_points 24 --d_model 100 --d_ff 400 --enc_layers 5 --dec_layers 5 --l2_coeff 1e-3 --dropout_ff .2 --dropout_emb .1 --d_qk 20 --d_v 20 --n_heads 6 --run_name spatiotemporal_al_solar --batch_size 32 --class_loss_imp 0 --initial_downsample_convs 1 --decay_factor .8 --warmup_steps 1000\".split()\n",
    "args = parser.parse_args(args)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default wandb log dir path of ./data/STF_LOG_DIR. This can be adjusted with the environment variable `STF_LOG_DIR`\n"
     ]
    }
   ],
   "source": [
    "log_dir = os.getenv(\"STF_LOG_DIR\")\n",
    "if log_dir is None:\n",
    "    log_dir = \"./data/STF_LOG_DIR\"\n",
    "print(\n",
    "    \"Using default wandb log dir path of ./data/STF_LOG_DIR. This can be adjusted with the environment variable `STF_LOG_DIR`\"\n",
    ")\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**in create_dset**\n",
    "\n",
    "1. CSVTimeSeires : csv time series : 전처리하는 class\n",
    "2. CSVTorchDset : dataset -> torch dataset 만드는 class\n",
    "3. Datamodule : Dataloader : from pl.lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     data_module,\n",
    "#     inv_scaler,\n",
    "#     scaler,\n",
    "#     null_val,\n",
    "#     plot_var_idxs,\n",
    "#     plot_var_names,\n",
    "#     pad_val,\n",
    "#     ) = create_dset(args)\n",
    "\n",
    "\n",
    "time_col_name = \"Datetime\"\n",
    "data_path = args.data_path\n",
    "time_features = [\"year\", \"month\", \"day\", \"weekday\", \"hour\", \"minute\"]\n",
    "\n",
    "data_path = \"./data/solar_AL_converted.csv\"\n",
    "target_cols = [str(i) for i in range(137)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = stf.data.CSVTimeSeries(\n",
    "        data_path=data_path,\n",
    "        target_cols=target_cols,\n",
    "        ignore_cols=\"all\",\n",
    "        time_col_name=time_col_name,\n",
    "        time_features=time_features,\n",
    "        val_split=0.2,\n",
    "        test_split=0.2,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-01 00:00:00</td>\n",
       "      <td>-0.802494</td>\n",
       "      <td>-0.713569</td>\n",
       "      <td>-0.698395</td>\n",
       "      <td>-0.700657</td>\n",
       "      <td>-0.791345</td>\n",
       "      <td>-0.696641</td>\n",
       "      <td>-0.722225</td>\n",
       "      <td>-0.698887</td>\n",
       "      <td>-0.701288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.713636</td>\n",
       "      <td>-0.701126</td>\n",
       "      <td>-0.786394</td>\n",
       "      <td>-0.694511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-01 00:10:00</td>\n",
       "      <td>-0.802494</td>\n",
       "      <td>-0.713569</td>\n",
       "      <td>-0.698395</td>\n",
       "      <td>-0.700657</td>\n",
       "      <td>-0.791345</td>\n",
       "      <td>-0.696641</td>\n",
       "      <td>-0.722225</td>\n",
       "      <td>-0.698887</td>\n",
       "      <td>-0.701288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.713636</td>\n",
       "      <td>-0.701126</td>\n",
       "      <td>-0.786394</td>\n",
       "      <td>-0.694511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.661017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-01 00:20:00</td>\n",
       "      <td>-0.802494</td>\n",
       "      <td>-0.713569</td>\n",
       "      <td>-0.698395</td>\n",
       "      <td>-0.700657</td>\n",
       "      <td>-0.791345</td>\n",
       "      <td>-0.696641</td>\n",
       "      <td>-0.722225</td>\n",
       "      <td>-0.698887</td>\n",
       "      <td>-0.701288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.713636</td>\n",
       "      <td>-0.701126</td>\n",
       "      <td>-0.786394</td>\n",
       "      <td>-0.694511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.322034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-01 00:30:00</td>\n",
       "      <td>-0.802494</td>\n",
       "      <td>-0.713569</td>\n",
       "      <td>-0.698395</td>\n",
       "      <td>-0.700657</td>\n",
       "      <td>-0.791345</td>\n",
       "      <td>-0.696641</td>\n",
       "      <td>-0.722225</td>\n",
       "      <td>-0.698887</td>\n",
       "      <td>-0.701288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.713636</td>\n",
       "      <td>-0.701126</td>\n",
       "      <td>-0.786394</td>\n",
       "      <td>-0.694511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-01-01 00:40:00</td>\n",
       "      <td>-0.802494</td>\n",
       "      <td>-0.713569</td>\n",
       "      <td>-0.698395</td>\n",
       "      <td>-0.700657</td>\n",
       "      <td>-0.791345</td>\n",
       "      <td>-0.696641</td>\n",
       "      <td>-0.722225</td>\n",
       "      <td>-0.698887</td>\n",
       "      <td>-0.701288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.713636</td>\n",
       "      <td>-0.701126</td>\n",
       "      <td>-0.786394</td>\n",
       "      <td>-0.694511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.355932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Datetime         0         1         2         3         4  \\\n",
       "0 2006-01-01 00:00:00 -0.802494 -0.713569 -0.698395 -0.700657 -0.791345   \n",
       "1 2006-01-01 00:10:00 -0.802494 -0.713569 -0.698395 -0.700657 -0.791345   \n",
       "2 2006-01-01 00:20:00 -0.802494 -0.713569 -0.698395 -0.700657 -0.791345   \n",
       "3 2006-01-01 00:30:00 -0.802494 -0.713569 -0.698395 -0.700657 -0.791345   \n",
       "4 2006-01-01 00:40:00 -0.802494 -0.713569 -0.698395 -0.700657 -0.791345   \n",
       "\n",
       "          5         6         7         8  ...       133       134       135  \\\n",
       "0 -0.696641 -0.722225 -0.698887 -0.701288  ... -0.713636 -0.701126 -0.786394   \n",
       "1 -0.696641 -0.722225 -0.698887 -0.701288  ... -0.713636 -0.701126 -0.786394   \n",
       "2 -0.696641 -0.722225 -0.698887 -0.701288  ... -0.713636 -0.701126 -0.786394   \n",
       "3 -0.696641 -0.722225 -0.698887 -0.701288  ... -0.713636 -0.701126 -0.786394   \n",
       "4 -0.696641 -0.722225 -0.698887 -0.701288  ... -0.713636 -0.701126 -0.786394   \n",
       "\n",
       "        136  Year  Month  Day  Weekday  Hour    Minute  \n",
       "0 -0.694511   0.0   -1.0 -1.0      1.0  -1.0 -1.000000  \n",
       "1 -0.694511   0.0   -1.0 -1.0      1.0  -1.0 -0.661017  \n",
       "2 -0.694511   0.0   -1.0 -1.0      1.0  -1.0 -0.322034  \n",
       "3 -0.694511   0.0   -1.0 -1.0      1.0  -1.0  0.016949  \n",
       "4 -0.694511   0.0   -1.0 -1.0      1.0  -1.0  0.355932  \n",
       "\n",
       "[5 rows x 144 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.train_data.head()\n",
    "\n",
    "#time col + target col : variables -> normalization sacaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dataloader 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_MODULE = stf.data.DataModule(\n",
    "        datasetCls=stf.data.CSVTorchDset,\n",
    "        dataset_kwargs={\n",
    "            \"csv_time_series\": dset,\n",
    "            \"context_points\": args.context_points,\n",
    "            \"target_points\": args.target_points,\n",
    "            \"time_resolution\": args.time_resolution,\n",
    "        },\n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.workers,\n",
    "        overfit=args.overfit, # arg는 from global parser\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(DATA_MODULE.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "4\n",
      "torch.Size([32, 168, 6])\n"
     ]
    }
   ],
   "source": [
    "print(args.batch_size)\n",
    "print(len(sample)) # elf._torch(ctxt_x, ctxt_y, trgt_x, trgt_y)\n",
    "print(sample[0].shape) #batch size X : context_len + pred_len : ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9333, -0.4783, -0.6610, -0.2727, -0.6667,  0.0000],\n",
       "        [ 0.9333, -0.4783, -0.3220, -0.2727, -0.6667,  0.0000],\n",
       "        [ 0.9333, -0.4783,  0.0169, -0.2727, -0.6667,  0.0000],\n",
       "        ...,\n",
       "        [ 1.0000, -0.2174,  0.3559, -0.2727, -0.3333,  0.0000],\n",
       "        [ 1.0000, -0.2174,  0.6949, -0.2727, -0.3333,  0.0000],\n",
       "        [ 1.0000, -0.1304, -1.0000, -0.2727, -0.3333,  0.0000]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INV_SCALER = lambda x: x\n",
    "# SCALER = lambda x: x\n",
    "NULL_VAL = None\n",
    "PLOT_VAR_IDXS = None\n",
    "PLOT_VAR_NAMES = None\n",
    "PAD_VAL = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##전처리 하는 것.\n",
    "INV_SCALER = dset.reverse_scaling\n",
    "SCALER = dset.apply_scaling\n",
    "NULL_VAL = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_module = DATA_MODULE\n",
    "inv_scaler = INV_SCALER\n",
    "scaler = SCALER\n",
    "null_val =NULL_VAL\n",
    "plot_var_idxs = PLOT_VAR_IDXS\n",
    "plot_var_names = PLOT_VAR_NAMES\n",
    "pad_val= PAD_VAL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Model\n",
    "args.null_value = null_val\n",
    "args.pad_value = pad_val\n",
    "# forecaster = create_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecaster\n",
      "\tL2: 0.001\n",
      "\tLinear Window: 0\n",
      "\tLinear Shared Weights: False\n",
      "\tRevIN: False\n",
      "\tDecomposition: False\n",
      "GlobalSelfAttn: AttentionLayer(\n",
      "  (inner_attention): PerformerAttention(\n",
      "    (kernel_fn): ReLU()\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (out_projection): Linear(in_features=120, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "GlobalCrossAttn: AttentionLayer(\n",
      "  (inner_attention): PerformerAttention(\n",
      "    (kernel_fn): ReLU()\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (out_projection): Linear(in_features=120, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "LocalSelfAttn: AttentionLayer(\n",
      "  (inner_attention): PerformerAttention(\n",
      "    (kernel_fn): ReLU()\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (out_projection): Linear(in_features=120, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "LocalCrossAttn: AttentionLayer(\n",
      "  (inner_attention): PerformerAttention(\n",
      "    (kernel_fn): ReLU()\n",
      "  )\n",
      "  (query_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (key_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (value_projection): Linear(in_features=100, out_features=120, bias=True)\n",
      "  (out_projection): Linear(in_features=120, out_features=100, bias=True)\n",
      "  (dropout_qkv): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Using Embedding: spatio-temporal\n",
      "Time Emb Dim: 6\n",
      "Space Embedding: True\n",
      "Time Embedding: True\n",
      "Val Embedding: True\n",
      "Given Embedding: True\n",
      "Null Value: None\n",
      "Pad Value: None\n",
      "Reconstruction Dropout: Timesteps 0.05, Standard 0.1, Seq (max len = 5) 0.2, Skip All Drop 1.0\n",
      " *** Spacetimeformer (v1.5) Summary: *** \n",
      "\t\tModel Dim: 100\n",
      "\t\tFF Dim: 400\n",
      "\t\tEnc Layers: 5\n",
      "\t\tDec Layers: 5\n",
      "\t\tEmbed Dropout: 0.1\n",
      "\t\tFF Dropout: 0.2\n",
      "\t\tAttn Out Dropout: 0.0\n",
      "\t\tAttn Matrix Dropout: 0.0\n",
      "\t\tQKV Dropout: 0.0\n",
      "\t\tL2 Coeff: 0.001\n",
      "\t\tWarmup Steps: 1000\n",
      "\t\tNormalization Scheme: batch\n",
      "\t\tAttention Time Windows: 1\n",
      "\t\tShifted Time Windows: False\n",
      "\t\tPosition Emb Type: abs\n",
      "\t\tRecon Loss Imp: 0.0\n",
      " ***                                  *** \n"
     ]
    }
   ],
   "source": [
    "#as to solar energy dataset\n",
    "x_dim, yc_dim, yt_dim = None, None, None\n",
    "x_dim = 6\n",
    "yc_dim = 137\n",
    "yt_dim = 137\n",
    "\n",
    "\n",
    "#config.model == \"spacetimeformer\":\n",
    "\n",
    "#setting for target length and context length\n",
    "if hasattr(args, \"context_points\") and hasattr(args, \"target_points\"):\n",
    "    max_seq_len = args.context_points + args.target_points\n",
    "elif hasattr(args, \"max_len\"):\n",
    "    max_seq_len = args.max_len\n",
    "else:\n",
    "    raise ValueError(\"Undefined max_seq_len\")\n",
    "\n",
    "\n",
    "config = args\n",
    "\n",
    "forecaster = stf.spacetimeformer_model.Spacetimeformer_Forecaster(\n",
    "    d_x=x_dim,\n",
    "    d_yc=yc_dim,\n",
    "    d_yt=yt_dim,\n",
    "    max_seq_len=max_seq_len,\n",
    "    start_token_len=config.start_token_len,\n",
    "\n",
    "\n",
    "    attn_factor=config.attn_factor,\n",
    "    d_model=config.d_model,\n",
    "    d_queries_keys=config.d_qk,\n",
    "    d_values=config.d_v,\n",
    "    n_heads=config.n_heads,\n",
    "    e_layers=config.enc_layers,\n",
    "    d_layers=config.dec_layers,\n",
    "    d_ff=config.d_ff,\n",
    "    dropout_emb=config.dropout_emb,\n",
    "    dropout_attn_out=config.dropout_attn_out,\n",
    "    dropout_attn_matrix=config.dropout_attn_matrix,\n",
    "    dropout_qkv=config.dropout_qkv,\n",
    "    dropout_ff=config.dropout_ff,\n",
    "    pos_emb_type=config.pos_emb_type,\n",
    "    use_final_norm=not config.no_final_norm,\n",
    "    global_self_attn=config.global_self_attn,\n",
    "    local_self_attn=config.local_self_attn,\n",
    "    global_cross_attn=config.global_cross_attn,\n",
    "    local_cross_attn=config.local_cross_attn,\n",
    "    performer_kernel=config.performer_kernel,\n",
    "    performer_redraw_interval=config.performer_redraw_interval,\n",
    "    attn_time_windows=config.attn_time_windows,\n",
    "    use_shifted_time_windows=config.use_shifted_time_windows,\n",
    "    norm=config.norm,\n",
    "    activation=config.activation,\n",
    "    init_lr=config.init_lr,\n",
    "    base_lr=config.base_lr,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    decay_factor=config.decay_factor,\n",
    "    initial_downsample_convs=config.initial_downsample_convs,\n",
    "    intermediate_downsample_convs=config.intermediate_downsample_convs,\n",
    "    embed_method=config.embed_method,\n",
    "    l2_coeff=config.l2_coeff,\n",
    "    loss=config.loss,\n",
    "    class_loss_imp=config.class_loss_imp,\n",
    "    recon_loss_imp=config.recon_loss_imp,\n",
    "    time_emb_dim=config.time_emb_dim,\n",
    "    null_value=config.null_value,\n",
    "    pad_value=config.pad_value,\n",
    "    linear_window=config.linear_window,\n",
    "    use_revin=config.use_revin,\n",
    "    linear_shared_weights=config.linear_shared_weights,\n",
    "    use_seasonal_decomp=config.use_seasonal_decomp,\n",
    "    use_val=not config.no_val,\n",
    "    use_time=not config.no_time,\n",
    "    use_space=not config.no_space,\n",
    "    use_given=not config.no_given,\n",
    "    recon_mask_skip_all=config.recon_mask_skip_all,\n",
    "    recon_mask_max_seq_len=config.recon_mask_max_seq_len,\n",
    "    recon_mask_drop_seq=config.recon_mask_drop_seq,\n",
    "    recon_mask_drop_standard=config.recon_mask_drop_standard,\n",
    "    recon_mask_drop_full=config.recon_mask_drop_full,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster.set_inv_scaler(inv_scaler) # scaler assigment in model\n",
    "forecaster.set_scaler(scaler)\n",
    "forecaster.set_null_value(null_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = create_callbacks(args, save_dir=log_dir)\n",
    "test_samples = next(iter(data_module.test_dataloader())) #한 sample만 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "4\n",
      "torch.Size([32, 168, 6])\n"
     ]
    }
   ],
   "source": [
    "print(args.batch_size)\n",
    "print(len(test_samples)) # elf._torch(ctxt_x, ctxt_y, trgt_x, trgt_y)\n",
    "print(test_samples[0].shape) #batch size X : context_len + pred_len : time_emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "{'val_check_interval': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(args.val_check_interval)\n",
    "\n",
    "#val_check_interval을 int로 변환\n",
    "if args.val_check_interval <= 1.0:\n",
    "        val_control = {\"val_check_interval\": args.val_check_interval}\n",
    "else:\n",
    "    val_control = {\"check_val_every_n_epoch\": int(args.val_check_interval)}\n",
    "\n",
    "print(val_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train epoch (전체 학습데이터)의 일부(여기서는 1이니까 전체)를 할때마다 validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often to check the validation set. Pass a float in the range [0.0, 1.0] to check after a fraction of the training epoch. Pass an int to check after a fixed number of training batches. An int value can only be higher than the number of training batches when check_val_every_n_epoch=None, which validates after every N training batches across epochs or during iteration-based training. Default: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeomss/miniconda3/envs/spacetimeformer/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "#pytorch lighting의 모델 정의 부분\n",
    "trainer = pl.Trainer(\n",
    "    gpus=args.gpus,\n",
    "    callbacks=callbacks,\n",
    "    # logger=logger if args.wandb else None,\n",
    "    # accelerator=\"dp\", # ->  gpu가 없어서...\n",
    "    \n",
    "    gradient_clip_val=args.grad_clip_norm,\n",
    "    gradient_clip_algorithm=\"norm\",\n",
    "    overfit_batches=20 if args.debug else 0,\n",
    "    accumulate_grad_batches=args.accumulate,\n",
    "    sync_batchnorm=True,\n",
    "    limit_val_batches=args.limit_val_batches,\n",
    "    **val_control,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(accumulate=1, activation='gelu', attn_factor=5, attn_plot=False, attn_time_windows=1, base_lr=0.0005, batch_size=32, class_loss_imp=0.0, context_points=168, d_ff=400, d_model=100, d_qk=20, d_v=20, data_path='auto', debug=False, dec_layers=5, decay_factor=0.8, dropout_attn_matrix=0.0, dropout_attn_out=0.0, dropout_emb=0.1, dropout_ff=0.2, dropout_qkv=0.0, dset='solar_energy', embed_method='spatio-temporal', enc_layers=5, global_cross_attn='performer', global_self_attn='performer', gpus=None, grad_clip_norm=0, init_lr=1e-10, initial_downsample_convs=1, intermediate_downsample_convs=0, l2_coeff=0.001, learning_rate=0.0001, limit_val_batches=1.0, linear_shared_weights=False, linear_window=0, local_cross_attn='performer', local_self_attn='performer', loss='mse', model='spacetimeformer', model_ckpt_dir='./data/STF_LOG_DIR/spatiotemporal_al_solar_7f10f866', n_heads=6, no_earlystopping=False, no_final_norm=False, no_given=False, no_space=False, no_time=False, no_val=False, norm='batch', null_value=None, overfit=False, pad_value=None, patience=5, performer_kernel='relu', performer_redraw_interval=100, plot=False, plot_samples=8, pos_emb_type='abs', recon_loss_imp=0.0, recon_mask_drop_full=0.05, recon_mask_drop_seq=0.2, recon_mask_drop_standard=0.1, recon_mask_max_seq_len=5, recon_mask_skip_all=1.0, run_name='spatiotemporal_al_solar', start_token_len=0, target_points=24, time_emb_dim=6, time_mask_anneal_steps=1000, time_mask_end=12, time_mask_loss=False, time_mask_start=1, time_resolution=1, trials=1, use_revin=False, use_seasonal_decomp=False, use_shifted_time_windows=False, val_check_interval=1.0, wandb=False, warmup_steps=1000, workers=6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "168\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(args.target_points)\n",
    "print(args.context_points)\n",
    "print(args.time_emb_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer.fit(forecaster, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "trainer.test(datamodule=data_module, ckpt_path=\"best\")\n",
    "\n",
    "# Predict (only here as a demo and test)\n",
    "forecaster.to(\"cuda\")\n",
    "xc, yc, xt, _ = test_samples\n",
    "yt_pred = forecaster.predict(xc, yc, xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # CLI\n",
    "#     parser = create_parser()\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     for trial in range(args.trials): # 여기서 지역 데이터를 계속 iteration 을 돌릴 것.\n",
    "#         main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacetimeformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
